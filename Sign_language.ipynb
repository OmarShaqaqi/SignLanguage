{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dldYHrIcqDEH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7nHl5VEqDCt"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dmdfQ_9sJhX",
        "outputId": "c93aa488-a874-46f7-e5d5-6e131566c4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.12.2)\n",
            "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULLu2Ru05QRO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "from torcheval.metrics import WordErrorRate\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import log_softmax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYHu-OGU9DPz",
        "outputId": "fb85d8b0-2f5a-42ff-a5bf-7ed1b437f189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wZFitIgqFmB"
      },
      "source": [
        "# Downloading training and testing folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv8KXz289oOo",
        "outputId": "c4671bd2-413d-44c1-9a3e-1809e68ef244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files extracted to /content/unziped_train/\n",
            "Files extracted to /content/unziped_test/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Path to the zip file on Google Drive (update this path)\n",
        "zip_file_path = '/content/drive/MyDrive/ICS471 - HW3/train.zip'\n",
        "\n",
        "# Destination folder where the unzipped content will be placed\n",
        "destination_folder = '/content/unziped_train/'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)\n",
        "\n",
        "print(f'Files extracted to {destination_folder}')\n",
        "\n",
        "\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/ICS471 - HW3/test.zip'\n",
        "\n",
        "\n",
        "destination_folder = '/content/unziped_test/'\n",
        "\n",
        "\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)\n",
        "\n",
        "print(f'Files extracted to {destination_folder}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmu3SFRhqKub"
      },
      "source": [
        "# processing the dataset and putting it in a valid form for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WE7QLh6LGab7",
        "outputId": "27b165dd-0460-477c-bb7b-78d29309733f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class NumericLabelDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Root directory with all video folders.\n",
        "            transform (callable, optional): Transform to apply to each frame.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.video_folders = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect all third-level folders and extract labels\n",
        "        for folder1 in sorted(os.listdir(root_dir)):\n",
        "            folder1_path = os.path.join(root_dir, folder1)\n",
        "            if os.path.isdir(folder1_path):\n",
        "                for folder2 in sorted(os.listdir(folder1_path)):\n",
        "                    folder2_path = os.path.join(folder1_path, folder2)\n",
        "                    print()\n",
        "                    if os.path.isdir(folder2_path) and len(os.listdir(folder2_path)) == 80:\n",
        "                        self.video_folders.append(folder2_path)\n",
        "                        # Extract label from folder name (e.g., '01_0001_(...)')\n",
        "                        label = int(folder1)  # Assuming label is in the second-level folder name\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_folders)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_folder = self.video_folders[idx]\n",
        "        frame_paths = sorted(glob.glob(os.path.join(video_folder, \"*.jpg\")))  # Adjust file extension if needed\n",
        "\n",
        "        frames = []\n",
        "        for frame_path in frame_paths:\n",
        "            image = Image.open(frame_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            frames.append(image)\n",
        "\n",
        "        # Stack frames into a tensor of shape (num_frames, channels, height, width)\n",
        "        video_tensor = torch.stack(frames)\n",
        "        label = self.labels[idx]\n",
        "        return video_tensor, label\n",
        "\n",
        "# Example Usage\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to MobileNetV2 input size\n",
        "    transforms.ToTensor(),          # Convert to PyTorch tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n",
        "\n",
        "train_dataset = NumericLabelDataset(root_dir=\"/content/unziped_train/train_full\", transform=transform)\n",
        "test_dataset = NumericLabelDataset(root_dir=\"/content/unziped_test/test\", transform=transform)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwqsNt7v-xxR"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98-YVXL5zaeT"
      },
      "outputs": [],
      "source": [
        "def build_vocab(sentences):\n",
        "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}  # Special tokens\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "# Example sentences from the uploaded file\n",
        "sentences = [\n",
        "    \"اسم الله\",\n",
        "    \"جميع الناس العرب\",\n",
        "    \"السلام عليكم ورحمة الله وبركاته\",\n",
        "    \"اليوم العالم يقدم برنامج أخر\",\n",
        "    \"موضوع دراستنا عن الإشارات التعليمية\",\n",
        "    \"كلمات اليوم مبتذلة في الدين\",\n",
        "    \"إبقى كلمات هادئة\",\n",
        "    \"لا تهتم\",\n",
        "    \"الله أكبر\"\n",
        "]\n",
        "\n",
        "vocab = build_vocab(sentences)\n",
        "filepath = \"/content/glove.6B.100d.txt\"\n",
        "\n",
        "\n",
        "def load_glove_embeddings(filepath, vocab, embedding_dim=100):\n",
        "    embeddings_index = {}\n",
        "\n",
        "    # Load pre-trained GloVe vectors\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    # Create embedding matrix for your vocabulary\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
        "    for i, word in enumerate(vocab):\n",
        "        if word in embeddings_index:\n",
        "            embedding_matrix[i] = embeddings_index[word]\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.uniform(-0.1, 0.1, embedding_dim)  # Random initialization\n",
        "\n",
        "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpxW8krr2ase",
        "outputId": "802ebc19-65cb-43ab-9741-f3dbf51f6725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: [1, 4, 5, 2], 2: [1, 6, 5, 2], 3: [1, 7, 8, 9, 10, 2], 4: [1, 11, 12, 13, 5, 14, 2], 5: [1, 15, 16, 17, 18, 19, 2], 6: [1, 20, 21, 22, 23, 24, 2], 7: [1, 25, 15, 26, 27, 28, 2], 8: [1, 29, 25, 30, 2], 9: [1, 31, 32, 5, 2], 10: [1, 5, 33, 2]}\n"
          ]
        }
      ],
      "source": [
        "def tokenize_sentence(sentence, vocab):\n",
        "    \"\"\"\n",
        "    Tokenize a sentence into a list of token indices based on the provided vocabulary.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The sentence to tokenize.\n",
        "        vocab (dict): A mapping from words to token indices.\n",
        "\n",
        "    Returns:\n",
        "        List[int]: A list of token indices representing the sentence.\n",
        "    \"\"\"\n",
        "    tokens = ['<sos>'] + sentence.split() + ['<eos>']  # Add <sos> and <eos> tokens\n",
        "    return [vocab.get(word, vocab['<unk>']) for word in tokens]\n",
        "\n",
        "# Example vocabulary\n",
        "vocab = {\n",
        "    '<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3,\n",
        "    'اسم': 4, 'الله': 5, 'الحمد': 6, 'جميع': 7, 'الصم': 8,\n",
        "    'العرب': 9, 'السامع': 10, 'السلام': 11, 'عليكم': 12, 'رحمة': 13,\n",
        "    'بركة': 14, 'اليوم': 15, 'اقدم': 16, 'انتم': 17, 'برنامج': 18,\n",
        "    'اخر': 19, 'موضوع': 20, 'دراسة': 21, 'لغة': 22, 'الاشارة': 23,\n",
        "    'العربية': 24, 'كلمات': 25, 'متفرقة': 26, 'في': 27, 'الدين': 28,\n",
        "    'ايضا': 29, 'عادية': 30, 'لا': 31, 'شرك': 32, 'اكبر': 33\n",
        "}\n",
        "\n",
        "\n",
        "# Mapping video numbers to sentences\n",
        "video_to_sentence = {\n",
        "    1: \"اسم الله\",\n",
        "    2: \"الحمد الله\",\n",
        "    3: \"جميع الصم العرب السامع\",\n",
        "    4: \"السلام عليكم رحمة الله بركة\",\n",
        "    5: \"اليوم اقدم انتم برنامج اخر\",\n",
        "    6: \"موضوع دراسة لغة الاشارة العربية\",\n",
        "    7: \"كلمات اليوم متفرقة في الدين\",\n",
        "    8: \"ايضا كلمات عادية\",\n",
        "    9: \"لا شرك الله\",\n",
        "    10: \"الله اكبر\"\n",
        "}\n",
        "\n",
        "\n",
        "# Tokenize each sentence\n",
        "video_to_tokens = {\n",
        "    video_id: tokenize_sentence(sentence, vocab)\n",
        "    for video_id, sentence in video_to_sentence.items()\n",
        "}\n",
        "\n",
        "# Print tokenized mapping\n",
        "print(video_to_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlvPiU4OUbru"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = load_glove_embeddings(filepath, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtTjASi_rmQG"
      },
      "source": [
        "# First Model Impementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daxwkxt_DQ60"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=False):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
        "                            bidirectional=bidirectional, batch_first=True)\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Input: (batch_size, seq_len, input_dim)\n",
        "        outputs, (hidden, cell) = self.lstm(features)\n",
        "        # Outputs: (batch_size, seq_len, hidden_dim * num_directions)\n",
        "        # Hidden: (num_layers * num_directions, batch_size, hidden_dim)\n",
        "        # Cell:   (num_layers * num_directions, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk540T280Ol4"
      },
      "outputs": [],
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, embedding_matrix=None):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding.weight.data.copy_(embedding_matrix)  # Load pre-trained embeddings\n",
        "            self.embedding.weight.requires_grad = False  # Freeze embeddings (optional)\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell):\n",
        "        \"\"\"\n",
        "        Forward pass of the decoder.\n",
        "\n",
        "        Args:\n",
        "            input_token (torch.Tensor): Current input token (batch_size,).\n",
        "            hidden (torch.Tensor): Hidden state from the previous time step.\n",
        "            cell (torch.Tensor): Cell state from the previous time step.\n",
        "\n",
        "        Returns:\n",
        "            output (torch.Tensor): Predicted probabilities for the next token (batch_size, vocab_size).\n",
        "            hidden (torch.Tensor): Updated hidden state.\n",
        "            cell (torch.Tensor): Updated cell state.\n",
        "        \"\"\"\n",
        "        # 1. Embed the input token\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)  # Shape: (batch_size, 1, embedding_dim)\n",
        "\n",
        "        # 2. Pass the embedded token through the LSTM\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # Output: (batch_size, 1, hidden_dim)\n",
        "\n",
        "        # 3. Predict the next token\n",
        "        prediction = self.fc_out(output.squeeze(1))  # Shape: (batch_size, vocab_size)\n",
        "\n",
        "        return prediction, hidden, cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc1VqVftLn-W"
      },
      "outputs": [],
      "source": [
        "class VideoClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # MobileNetV2 as feature extractor\n",
        "        self.feature_extractor = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            *list(self.feature_extractor.features),  # Keep the feature layers\n",
        "            nn.AdaptiveAvgPool2d((1, 1))  # Add global average pooling\n",
        "        )\n",
        "\n",
        "    def forward(self, video):\n",
        "        batch_size, num_frames, C, H, W = video.size()  # (B, 80, 3, 224, 224)\n",
        "\n",
        "        # Reshape to process each frame individually\n",
        "        video = video.view(-1, C, H, W)  # (B * num_frames, 3, 224, 224)\n",
        "\n",
        "        # Extract features\n",
        "        with torch.no_grad():\n",
        "            features = self.feature_extractor(video)  # (B * num_frames, 1280, 1, 1)\n",
        "\n",
        "        # Remove the last two dimensions (1, 1) from global pooling\n",
        "        features = features.view(batch_size, num_frames, -1)  # (B, 80, 1280)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw2eHP2y1Bdo"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def process_batch_trg(batch_labels, video_to_tokens, vocab):\n",
        "    \"\"\"\n",
        "    Convert a batch of video labels into padded tokenized sequences.\n",
        "\n",
        "    Args:\n",
        "        batch_labels (torch.Tensor): A batch of video labels (e.g., [1, 3, 2, 5, 8])\n",
        "        video_to_tokens (dict): A dictionary mapping video IDs to tokenized sequences.\n",
        "        vocab (dict): Vocabulary mapping words to indices.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Padded tokenized sequences (batch_size, max_seq_len).\n",
        "    \"\"\"\n",
        "    # Convert video labels to tokenized sequences\n",
        "    tokenized_sequences = [torch.tensor(video_to_tokens[label.item()]) for label in batch_labels]\n",
        "\n",
        "    # Pad the sequences to the same length (max_seq_len)\n",
        "    padded_sequences = pad_sequence(tokenized_sequences, batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    return padded_sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4M5a6_3ryGb"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0z4YXFxx0mL",
        "outputId": "e0c9bd62-c6bf-4b9b-f22b-c6848f63896f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 218MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.9590\n",
            "Epoch 2/10, Loss: 0.2950\n",
            "Epoch 3/10, Loss: 0.2284\n",
            "Epoch 4/10, Loss: 0.2012\n",
            "Epoch 5/10, Loss: 0.1394\n",
            "Epoch 6/10, Loss: 0.1085\n",
            "Epoch 7/10, Loss: 0.0653\n",
            "Epoch 8/10, Loss: 0.0515\n",
            "Epoch 9/10, Loss: 0.0336\n",
            "Epoch 10/10, Loss: 0.0264\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Loss function and optimizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoder = EncoderLSTM(input_dim=1280 , hidden_dim=512).to(device)\n",
        "decoder = DecoderLSTM(vocab_size=len(vocab), embedding_dim=100, hidden_dim=512).to(device)\n",
        "feautre_extractor = VideoClassificationModel().to(device)\n",
        "\n",
        "Epoch = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "\n",
        "teacher_forcing_ratio = 1 # Probability of using ground truth token as input\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(Epoch):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in train_dataloader:\n",
        "        # Step 1: Process `trg` (convert video IDs to tokenized sequences)\n",
        "        trg = process_batch_trg(trg, video_to_tokens, vocab)\n",
        "\n",
        "        # Step 2: Move data to device\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # Step 3: Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        feature = feautre_extractor(src)\n",
        "\n",
        "        # Step 4: Forward pass through encoder\n",
        "        output,(hidden, cell) = encoder(feature)\n",
        "\n",
        "        # Step 5: Initialize decoder input with <sos> token\n",
        "        input_token = trg[:, 0]  # First token in every sequence (batch_size,)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        # Step 6: Loop through the target sequence\n",
        "        for t in range(1, trg.size(1)):  # Start from the second token\n",
        "            output, hidden, cell = decoder(input_token, hidden,cell)  # Forward pass through decoder\n",
        "\n",
        "            # Compute loss\n",
        "            loss += criterion(output, trg[:, t])  # Compare output with ground truth token\n",
        "\n",
        "            # Teacher forcing: Use ground truth or predicted token as next input\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input_token = trg[:, t] if teacher_force else output.argmax(1)\n",
        "\n",
        "        # Step 7: Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Normalize loss by sequence length and accumulate\n",
        "        epoch_loss += loss.item() / trg.size(1)\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{Epoch}, Loss: {epoch_loss / len(train_dataloader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob99S11q-8Ef"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_with_wer(encoder, decoder,extractor,test_dataloader, video_to_tokens, vocab, max_len=50):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set and compute WER using TorchEval.\n",
        "\n",
        "    Args:\n",
        "        encoder (nn.Module): Encoder model.\n",
        "        decoder (nn.Module): Decoder model.\n",
        "        test_dataloader (DataLoader): Dataloader for the test set.\n",
        "        video_to_tokens (dict): Mapping of video IDs to tokenized sequences.\n",
        "        vocab (dict): Vocabulary mapping indices to words.\n",
        "        max_len (int): Maximum length for generated sentences.\n",
        "\n",
        "    Returns:\n",
        "        float: Average WER across the test set.\n",
        "    \"\"\"\n",
        "    reverse_vocab = {idx: word for word, idx in vocab.items()}  # Reverse vocab for decoding\n",
        "    wer_metric = WordErrorRate()  # Initialize WER metric\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg_labels in test_dataloader:\n",
        "            # Move data to device\n",
        "            src, trg_labels = src.to(device), trg_labels.to(device)\n",
        "\n",
        "            # Convert ground truth labels to tokenized sentences\n",
        "            trg = process_batch_trg(trg_labels, video_to_tokens, vocab)\n",
        "\n",
        "            feature = extractor(src)\n",
        "            # Forward pass through encoder\n",
        "            encoder_outputs, (hidden, cell) = encoder(feature)\n",
        "\n",
        "            # Initialize input token (<sos>)\n",
        "            input_token = torch.tensor([vocab['<sos>']] * src.size(0)).to(device)\n",
        "\n",
        "            # Generate predictions\n",
        "            predictions = []\n",
        "            for _ in range(max_len):\n",
        "                output, hidden, cell = decoder(input_token, hidden, cell)\n",
        "                top1 = output.argmax(1)  # Get most probable token\n",
        "                predictions.append(top1)\n",
        "                input_token = top1  # Use predicted token as next input\n",
        "\n",
        "            # Stack predictions into sequence\n",
        "            predictions = torch.stack(predictions, dim=1)  # (batch_size, max_len)\n",
        "\n",
        "            # Convert predictions and ground truths to strings\n",
        "            predicted_sentences = [\n",
        "                \" \".join([reverse_vocab[idx.item()] for idx in prediction if idx.item() not in {vocab['<pad>'], vocab['<sos>'], vocab['<eos>']}])\n",
        "                for prediction in predictions\n",
        "            ]\n",
        "            reference_sentences = [\n",
        "                \" \".join([reverse_vocab[idx.item()] for idx in reference if idx.item() not in {vocab['<pad>'], vocab['<sos>'], vocab['<eos>']}])\n",
        "                for reference in trg\n",
        "            ]\n",
        "\n",
        "            # Update WER metric with current batch\n",
        "            wer_metric.update(predicted_sentences, reference_sentences)\n",
        "\n",
        "    # Compute the final WER\n",
        "    return wer_metric.compute()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFXzOn6W_OKy"
      },
      "outputs": [],
      "source": [
        "acc = evaluate_model_with_wer(encoder,decoder,feautre_extractor,test_dataloader,video_to_tokens,vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ipYOUz4_pPv"
      },
      "outputs": [],
      "source": [
        "decoder1 = decoder.to(device)\n",
        "encoder1 = encoder.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "collapsed": true,
        "id": "dBLCd8J6FIHz",
        "outputId": "3b0f3519-79a5-4aa4-daeb-ad85a5ba9eaf"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Parent directory content does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6e1f118cf6a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content/decoder_1.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content/encoder_1.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory content does not exist."
          ]
        }
      ],
      "source": [
        "torch.save(decoder1.state_dict(), 'decoder_1.pth')\n",
        "torch.save(encoder1.state_dict(), 'encoder_1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C93dg_XAnkI",
        "outputId": "ef1c7619-389c-4f89-d2a8-011d33eec1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER of an LSTM model : 0.9788135886192322 \n"
          ]
        }
      ],
      "source": [
        "print(f\"WER of an LSTM model : {acc} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egmI-Eb1euUc"
      },
      "source": [
        "# Trying more Layers to check if it will help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "08nF2139ehiT",
        "outputId": "1a699723-58f7-4e35-ce24-46a999e3a2a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 1.0780\n",
            "Epoch 2/10, Loss: 0.3207\n",
            "Epoch 3/10, Loss: 0.2860\n",
            "Epoch 4/10, Loss: 0.2549\n",
            "Epoch 5/10, Loss: 0.2246\n",
            "Epoch 6/10, Loss: 0.2002\n",
            "Epoch 7/10, Loss: 0.1473\n",
            "Epoch 8/10, Loss: 0.1455\n",
            "Epoch 9/10, Loss: 0.0984\n",
            "Epoch 10/10, Loss: 0.1151\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Loss function and optimizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoder = EncoderLSTM(input_dim=1280 ,num_layers=2, hidden_dim=512).to(device)\n",
        "decoder = DecoderLSTM(vocab_size=len(vocab),num_layers = 2, embedding_dim=100, hidden_dim=512).to(device)\n",
        "feautre_extractor = VideoClassificationModel().to(device)\n",
        "\n",
        "Epoch = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "\n",
        "teacher_forcing_ratio = 1 # Probability of using ground truth token as input\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(Epoch):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in train_dataloader:\n",
        "        # Step 1: Process `trg` (convert video IDs to tokenized sequences)\n",
        "        trg = process_batch_trg(trg, video_to_tokens, vocab)\n",
        "\n",
        "        # Step 2: Move data to device\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # Step 3: Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        feature = feautre_extractor(src)\n",
        "\n",
        "        # Step 4: Forward pass through encoder\n",
        "        output,(hidden, cell) = encoder(feature)\n",
        "\n",
        "        # Step 5: Initialize decoder input with <sos> token\n",
        "        input_token = trg[:, 0]  # First token in every sequence (batch_size,)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        # Step 6: Loop through the target sequence\n",
        "        for t in range(1, trg.size(1)):  # Start from the second token\n",
        "            output, hidden, cell = decoder(input_token, hidden,cell)  # Forward pass through decoder\n",
        "\n",
        "            # Compute loss\n",
        "            loss += criterion(output, trg[:, t])  # Compare output with ground truth token\n",
        "\n",
        "            # Teacher forcing: Use ground truth or predicted token as next input\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input_token = trg[:, t] if teacher_force else output.argmax(1)\n",
        "\n",
        "        # Step 7: Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Normalize loss by sequence length and accumulate\n",
        "        epoch_loss += loss.item() / trg.size(1)\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{Epoch}, Loss: {epoch_loss / len(train_dataloader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RHvdsp5Te1m6",
        "outputId": "99f9cf50-dda1-45af-f393-1cad000f0943"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'evaluate_model_with_wer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2ebb2dc37dc1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_with_wer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeautre_extractor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvideo_to_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model_with_wer' is not defined"
          ]
        }
      ],
      "source": [
        "acc = evaluate_model_with_wer(encoder,decoder,feautre_extractor,test_dataloader,video_to_tokens,vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IiW8JWyBe4Jb"
      },
      "outputs": [],
      "source": [
        "print(f\"WER of an LSTM model with 2 Layers : {acc} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiGzRepMluje"
      },
      "source": [
        "# With Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK7D2xPulxre"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Compute dot-product attention.\n",
        "\n",
        "        Args:\n",
        "            hidden (torch.Tensor): Decoder hidden state (batch_size, decoder_hidden_dim).\n",
        "            encoder_outputs (torch.Tensor): Encoder outputs (batch_size, seq_len, encoder_hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            context (torch.Tensor): Context vector (batch_size, encoder_hidden_dim).\n",
        "            attention_weights (torch.Tensor): Attention weights (batch_size, seq_len).\n",
        "        \"\"\"\n",
        "        # Compute dot product between hidden state and encoder outputs\n",
        "        # hidden: (batch_size, decoder_hidden_dim)\n",
        "        # encoder_outputs: (batch_size, seq_len, encoder_hidden_dim)\n",
        "        if hidden.dim() == 3 and hidden.size(0) == 1:\n",
        "            hidden = hidden.squeeze(0)\n",
        "        attention_scores = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)  / (hidden.size(-1) ** 0.5)# (batch_size, seq_len)\n",
        "\n",
        "        # Normalize scores with softmax\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
        "\n",
        "        # Compute context vector as weighted sum of encoder outputs\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (batch_size, encoder_hidden_dim)\n",
        "\n",
        "        return context, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Lt3L8AmXZO"
      },
      "outputs": [],
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, embedding_matrix = None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding.weight.data.copy_(embedding_matrix)  # Load pre-trained embeddings\n",
        "            self.embedding.weight.requires_grad = False  # Freeze embeddings (optional)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_dim + encoder_hidden_dim, decoder_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc_out = nn.Linear(decoder_hidden_dim, vocab_size)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = Attention()\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder with attention.\n",
        "        \"\"\"\n",
        "        # Ensure hidden and cell have the correct shape\n",
        "        if hidden.dim() == 4:\n",
        "            hidden = hidden.squeeze(0)  # Remove the extra dimension\n",
        "        if cell.dim() == 4:\n",
        "            cell = cell.squeeze(0)\n",
        "\n",
        "        # Embed the input token\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)  # (batch_size, 1, embedding_dim)\n",
        "\n",
        "        # Compute attention\n",
        "        context, attention_weights = self.attention(hidden, encoder_outputs)\n",
        "\n",
        "        # Concatenate context and embedded input\n",
        "        lstm_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)  # (batch_size, 1, embedding_dim + encoder_hidden_dim)\n",
        "\n",
        "\n",
        "        # Pass through LSTM\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "        # Predict next token\n",
        "        prediction = self.fc_out(output.squeeze(1))  # (batch_size, vocab_size)\n",
        "\n",
        "        return prediction, hidden, cell, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BDVfhEasYrG"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq9btp2FmYQW",
        "outputId": "a7e1fd17-4685-4391-8ade-7adf971f2fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 2.0175\n",
            "Epoch 2/10, Loss: 1.6782\n",
            "Epoch 3/10, Loss: 1.4863\n",
            "Epoch 4/10, Loss: 1.3510\n",
            "Epoch 5/10, Loss: 1.2655\n",
            "Epoch 6/10, Loss: 1.1841\n",
            "Epoch 7/10, Loss: 1.2468\n",
            "Epoch 8/10, Loss: 1.0771\n",
            "Epoch 9/10, Loss: 1.0831\n",
            "Epoch 10/10, Loss: 1.0700\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Loss function and optimizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoder = EncoderLSTM(input_dim=1280, hidden_dim=512).to(device)\n",
        "decoder = DecoderWithAttention(vocab_size=len(vocab),\n",
        "                               embedding_dim=100,\n",
        "                               encoder_hidden_dim=512,\n",
        "                               decoder_hidden_dim=512,\n",
        "                               embedding_matrix=embedding_matrix).to(device)\n",
        "feautre_extractor = VideoClassificationModel().to(device)\n",
        "\n",
        "Epoch = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = optim.Adam(params, lr=0.01)\n",
        "\n",
        "teacher_forcing_ratio = 0.5 # Probability of using ground truth token as input\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(Epoch):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in train_dataloader:\n",
        "        # Step 1: Process `trg` (convert video IDs to tokenized sequences)\n",
        "        trg = process_batch_trg(trg, video_to_tokens, vocab)\n",
        "\n",
        "        # Step 2: Move data to device\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # Step 3: Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        feature = feautre_extractor(src)\n",
        "\n",
        "        # Step 4: Forward pass through encoder\n",
        "        encoder_output,(hidden, cell) = encoder(feature)\n",
        "\n",
        "        # Step 5: Initialize decoder input with <sos> token\n",
        "        input_token = trg[:, 0]  # First token in every sequence (batch_size,)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        # Step 6: Loop through the target sequence\n",
        "        for t in range(1, trg.size(1)):  # Start from the second token\n",
        "\n",
        "            decoder_output, hidden, cell, attention = decoder(input_token, hidden,cell,encoder_output)  # Forward pass through decoder\n",
        "\n",
        "            # Compute loss\n",
        "            loss += criterion(decoder_output, trg[:, t])  # Compare output with ground truth token\n",
        "\n",
        "            # Teacher forcing: Use ground truth or predicted token as next input\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input_token = trg[:, t] if teacher_force else decoder_output.argmax(1)\n",
        "\n",
        "        # Step 7: Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Normalize loss by sequence length and accumulate\n",
        "        epoch_loss += loss.item() / trg.size(1)\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{Epoch}, Loss: {epoch_loss / len(train_dataloader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW7k-sKCscXd"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIcx1z4X35Hn"
      },
      "outputs": [],
      "source": [
        "from torcheval.metrics import WordErrorRate\n",
        "\n",
        "def evaluate_model_attention_with_wer(encoder, decoder,extractor,test_dataloader, video_to_tokens, vocab, max_len=50):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set and compute WER using TorchEval.\n",
        "\n",
        "    Args:\n",
        "        encoder (nn.Module): Encoder model.\n",
        "        decoder (nn.Module): Decoder model.\n",
        "        test_dataloader (DataLoader): Dataloader for the test set.\n",
        "        video_to_tokens (dict): Mapping of video IDs to tokenized sequences.\n",
        "        vocab (dict): Vocabulary mapping indices to words.\n",
        "        max_len (int): Maximum length for generated sentences.\n",
        "\n",
        "    Returns:\n",
        "        float: Average WER across the test set.\n",
        "    \"\"\"\n",
        "    reverse_vocab = {idx: word for word, idx in vocab.items()}  # Reverse vocab for decoding\n",
        "    wer_metric = WordErrorRate()  # Initialize WER metric\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg_labels in test_dataloader:\n",
        "            # Move data to device\n",
        "            src, trg_labels = src.to(device), trg_labels.to(device)\n",
        "\n",
        "            # Convert ground truth labels to tokenized sentences\n",
        "            trg = process_batch_trg(trg_labels, video_to_tokens, vocab)\n",
        "\n",
        "            feature = extractor(src)\n",
        "            # Forward pass through encoder\n",
        "            encoder_outputs, (hidden, cell) = encoder(feature)\n",
        "\n",
        "            # Initialize input token (<sos>)\n",
        "            input_token = torch.tensor([vocab['<sos>']] * src.size(0)).to(device)\n",
        "\n",
        "            # Generate predictions\n",
        "            predictions = []\n",
        "            for _ in range(max_len):\n",
        "                output, hidden, cell,att = decoder(input_token, hidden, cell,encoder_outputs)\n",
        "                top1 = output.argmax(1)  # Get most probable token\n",
        "                predictions.append(top1)\n",
        "                input_token = top1  # Use predicted token as next input\n",
        "\n",
        "            # Stack predictions into sequence\n",
        "            predictions = torch.stack(predictions, dim=1)  # (batch_size, max_len)\n",
        "\n",
        "            # Convert predictions and ground truths to strings\n",
        "            predicted_sentences = [\n",
        "                \" \".join([reverse_vocab[idx.item()] for idx in prediction if idx.item() not in {vocab['<pad>'], vocab['<sos>'], vocab['<eos>']}])\n",
        "                for prediction in predictions\n",
        "            ]\n",
        "            reference_sentences = [\n",
        "                \" \".join([reverse_vocab[idx.item()] for idx in reference if idx.item() not in {vocab['<pad>'], vocab['<sos>'], vocab['<eos>']}])\n",
        "                for reference in trg\n",
        "            ]\n",
        "\n",
        "            # Update WER metric with current batch\n",
        "            wer_metric.update(predicted_sentences, reference_sentences)\n",
        "\n",
        "    # Compute the final WER\n",
        "    return wer_metric.compute()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6enPAJBPFniA"
      },
      "outputs": [],
      "source": [
        "decoder2 = decoder.to(device)\n",
        "encoder2 = encoder.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptFh_W8HFqct"
      },
      "outputs": [],
      "source": [
        "torch.save(decoder2.state_dict(), 'decoder_2.pth')\n",
        "torch.save(encoder2.state_dict(), 'encoder_2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS7aqZ6zzzfk"
      },
      "outputs": [],
      "source": [
        "acc1 = evaluate_model_attention_with_wer(encoder,decoder,feautre_extractor,test_dataloader,video_to_tokens,vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU6FUegB4hAF",
        "outputId": "c555f24b-2ed4-4036-91e8-003d02d3c611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER on LSTM with Attention : 1.0677965879440308\n"
          ]
        }
      ],
      "source": [
        "print(f\"WER on LSTM with Attention : {acc1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKK1Hpt25uUE"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96oIKJl-p29C"
      },
      "outputs": [],
      "source": [
        "class TransformerVideoToText(nn.Module):\n",
        "    def __init__(self, feature_dim, vocab_size, embed_dim, num_heads, num_encoder_layers, num_decoder_layers, dropout=0.1, max_seq_len=80):\n",
        "        super(TransformerVideoToText, self).__init__()\n",
        "\n",
        "        # Embedding for tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Positional Encoding for video features and token embeddings\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
        "\n",
        "        # Linear layer to map video features to the embedding dimension\n",
        "        self.feature_projection = nn.Linear(feature_dim, embed_dim)\n",
        "\n",
        "        # Transformer\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        # Linear layer for vocabulary prediction\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src (Tensor): Encoder inputs (batch_size, seq_len, feature_dim).\n",
        "            trg (Tensor): Decoder inputs (batch_size, seq_len).\n",
        "        Returns:\n",
        "            Tensor: Log probabilities for each token (batch_size, seq_len, vocab_size).\n",
        "        \"\"\"\n",
        "        # Project video features to the embedding dimension\n",
        "        src = self.feature_projection(src)\n",
        "\n",
        "        # Add positional encoding to the encoder inputs\n",
        "        src = self.dropout(src + self.positional_encoding[:, :src.size(1), :])\n",
        "\n",
        "        # Add positional encoding to the decoder inputs\n",
        "        trg = self.embedding(trg) + self.positional_encoding[:, :trg.size(1), :]\n",
        "        trg = self.dropout(trg)\n",
        "\n",
        "        # Transformer forward pass\n",
        "        transformer_out = self.transformer(src, trg)\n",
        "\n",
        "        # Output predictions\n",
        "        output = self.fc_out(transformer_out)\n",
        "        return log_softmax(output, dim=-1)  # Log probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzBZvzel5wgk",
        "outputId": "bc2831b5-0ab0-4dcf-d37a-ae46fbd2b355"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 79.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 1.0660\n",
            "Epoch 2/10, Loss: 0.0131\n",
            "Epoch 3/10, Loss: 0.0064\n",
            "Epoch 4/10, Loss: 0.0044\n",
            "Epoch 5/10, Loss: 0.0033\n",
            "Epoch 6/10, Loss: 0.0026\n",
            "Epoch 7/10, Loss: 0.0022\n",
            "Epoch 8/10, Loss: 0.0018\n",
            "Epoch 9/10, Loss: 0.0015\n",
            "Epoch 10/10, Loss: 0.0013\n"
          ]
        }
      ],
      "source": [
        "# Loss function and optimizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define model\n",
        "feature_extractor = VideoClassificationModel().to(device)\n",
        "transformer_model = TransformerVideoToText(\n",
        "    feature_dim=1280 ,\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=512,\n",
        "    num_heads=8,\n",
        "    num_encoder_layers=4,\n",
        "    num_decoder_layers=4,\n",
        "    dropout=0.1,\n",
        "    max_seq_len=80\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.0001)\n",
        "\n",
        "teacher_forcing_ratio = 0.5  # Probability of using ground truth token as input\n",
        "\n",
        "# Training Loop\n",
        "Epoch = 10\n",
        "for epoch in range(Epoch):\n",
        "    transformer_model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in train_dataloader:\n",
        "\n",
        "        # Step 1: Process `trg` (convert video IDs to tokenized sequences)\n",
        "        trg = process_batch_trg(trg, video_to_tokens, vocab)\n",
        "\n",
        "        # Step 2: Move data to device\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "\n",
        "        feature = feature_extractor(src)\n",
        "\n",
        "        # Step 3: Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Step 4: Prepare input and target sequences\n",
        "        trg_input = trg[:, :-1]  # Input tokens for the decoder (<sos> to last token)\n",
        "        trg_output = trg[:, 1:]  # Target tokens for loss computation (second token to <eos>)\n",
        "\n",
        "        # Step 5: Forward pass\n",
        "        outputs = transformer_model(feature, trg_input)\n",
        "\n",
        "        # Step 6: Compute loss\n",
        "        outputs = outputs.reshape(-1, outputs.size(-1))  # Flatten outputs for loss computation\n",
        "        trg_output = trg_output.reshape(-1)  # Flatten targets\n",
        "        loss = criterion(outputs, trg_output)\n",
        "\n",
        "        # Step 7: Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Normalize loss by sequence length and accumulate\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch + 1}/{Epoch}, Loss: {epoch_loss / len(train_dataloader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QArXTDd1KBM"
      },
      "outputs": [],
      "source": [
        "from torcheval.metrics import WordErrorRate\n",
        "\n",
        "def test_model(transformer_model, feature_extractor, test_dataloader, vocab, video_to_tokens, max_len=50):\n",
        "    \"\"\"\n",
        "    Test the Transformer model on the test set and compute WER.\n",
        "\n",
        "    Args:\n",
        "        transformer_model (nn.Module): Transformer model.\n",
        "        feature_extractor (nn.Module): Video feature extractor.\n",
        "        test_dataloader (DataLoader): Dataloader for the test set.\n",
        "        vocab (dict): Vocabulary mapping tokens to indices.\n",
        "        video_to_tokens (dict): Mapping from video IDs to tokenized sequences.\n",
        "        max_len (int): Maximum sequence length for decoding.\n",
        "\n",
        "    Returns:\n",
        "        float: Word Error Rate (WER) on the test set.\n",
        "    \"\"\"\n",
        "    reverse_vocab = {idx: word for word, idx in vocab.items()}  # Reverse vocab for decoding\n",
        "    wer_metric = WordErrorRate()  # Initialize WER metric\n",
        "\n",
        "    transformer_model.eval()\n",
        "    feature_extractor.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in test_dataloader:\n",
        "            # Step 1: Move data to device\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            # Step 2: Extract features and reshape `src`\n",
        "            feature = feature_extractor(src)  # (batch_size, channels, height, width)\n",
        "            batch_size, frames, features = feature.size()\n",
        "\n",
        "            src = feature.view(batch_size, frames, features)  # Reshape to (batch_size, seq_len, feature_dim)\n",
        "\n",
        "            # Step 3: Convert `trg` to tokenized sequences\n",
        "            trg = process_batch_trg(trg, video_to_tokens, vocab)\n",
        "\n",
        "            # Step 4: Initialize decoding variables\n",
        "            input_token = torch.tensor([vocab['<sos>']] * batch_size).unsqueeze(1).to(device)  # Start with <sos>\n",
        "            predictions = []\n",
        "\n",
        "            # Step 5: Generate sequences (greedy decoding)\n",
        "            for _ in range(max_len):\n",
        "                outputs = transformer_model(src, input_token)  # Forward pass\n",
        "                next_token = outputs[:, -1, :].argmax(dim=-1).unsqueeze(1)  # Get most probable token\n",
        "                predictions.append(next_token)\n",
        "                input_token = torch.cat([input_token, next_token], dim=1)  # Append token\n",
        "\n",
        "                # Stop decoding if all sequences generate <eos>\n",
        "                if (next_token == vocab['<eos>']).all():\n",
        "                    break\n",
        "\n",
        "            # Step 6: Stack predictions and remove special tokens\n",
        "            predictions = torch.cat(predictions, dim=1)  # (batch_size, seq_len)\n",
        "            predicted_sentences = [\n",
        "                \" \".join([reverse_vocab[idx.item()] for idx in pred if idx.item() not in {vocab['<pad>'], vocab['<sos>'], vocab['<eos>']}])\n",
        "                for pred in predictions\n",
        "            ]\n",
        "            reference_sentences = [\n",
        "                \" \".join([reverse_vocab[idx.item()] for idx in ref if idx.item() not in {vocab['<pad>'], vocab['<sos>'], vocab['<eos>']}])\n",
        "                for ref in trg\n",
        "            ]\n",
        "\n",
        "            # Step 7: Update WER metric\n",
        "            wer_metric.update(predicted_sentences, reference_sentences)\n",
        "\n",
        "    # Compute final WER\n",
        "    return wer_metric.compute()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XEhvDI_1fyD"
      },
      "outputs": [],
      "source": [
        "acc2 = test_model(transformer_model,feature_extractor,test_dataloader,vocab,video_to_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK37GqCl1rSo",
        "outputId": "db48cec4-b3c1-4df3-8172-5d2ab9fd6672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The WER for the Transformer is: 1.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"The WER for the Transformer is: {acc2}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}